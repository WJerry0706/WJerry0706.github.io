<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Haokun Zhu</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Haokun Zhu</name>
              </p>
             <p>Hi! I am a senior student from <strong>Shanghai Jiao Tong University (SJTU)</strong>.
               Currently, I'm working with <strong><a href="https://scholar.google.com/citations?hl=en&user=Jm5qsAYAAAAJ/">Ph.D. Teng Hu</a></strong> as an undergraduate researcher at SJTU, supervised by <strong><a href="https://yiranran.github.io/">Prof. Ran Yi</a></strong>.
               My research interests mainly lie in <strong>Computer Vision</strong> and <strong>Generative Models</strong>.
              </p>
              <p style="text-align:center">
                <a href="mailto: zhuhaokun@sjtu.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/resume_zhk.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=1MJRbuYAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/zwandering">Github</a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:60%;max-width:60%;margin-left:90%;text-align:right;">
              <a href="images/me.jpg"><img style="width:75%;max-width:75%" alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am currently an undergraduate researcher in Digital Media & Computer Vision Laboratory(<a href="https://dmcv.sjtu.edu.cn/">DMCV</a>) at SJTU advised by <strong><a href="https://yiranran.github.io/">Prof. Ran Yi</a></strong>.
                I'm interested in deep generative models like GANs and Diffusion Models. My dream is to develop generative models with strong capabilities and employ them in order to bring benefits to the every-day life of everyone.
                <p style="text-align:justify">
                    In my past one year of research experience, I have explored a wide range of directions, including:
                    <li style="line-height:125%">
                      <strong>Few-shot Image Generation with Diffusion Model</strong>:
                      how to employ diffusion model in producing high-quality and diverse images in a new domain with only a small number of training data.
                    </li>
                    <li style="line-height:125%">
                      <strong>Aesthetic Guided Universal Style Transfer</strong>:
                      how to transfer the style of an arbitrary image to another content image while striking a balance among aesthetic qualities, style transfromation and content presevation.
                    </li>
                    <li style="line-height:125%">
                      <strong>Stroke-based Neural Painting</strong>:
                      how to recreate a pixel-based image with a set of brushstrokes like real human-beings while achieving both faithful reconstruction and stroke style at the same time.
                    </li>
                    <li style="line-height:125%">
                      <strong>Image Vectorization</strong>:
                      how to transform raster images into scalable vector graphics which have superior adaptability and detailed representation.
                    </li>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
              <strong>[2023.07.26]:</strong> Our paper <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611766">Stroke-based Neural Painting and Stylization with Dynamically Predicted Painting Region</a> accepted by ACM MM 2023!
              </p>
              <p>
              <strong>[2023.07.14]:</strong> Our paper <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_paper.html">Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption</a> accepted by ICCV 2023!
              </p>
              
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Publications & Preprints (* means equal contribution)</heading>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cvpr.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href=""><papertitle>AesStyler: Aesthetic Guided Universal Style Transfer</papertitle></a>
              </a>
              <br>
              <strong>Haokun Zhu*</strong>,
              <a href="https://yiranran.github.io/">Ran Yi</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=Jm5qsAYAAAAJ/">Teng Hu</a>,
              <a href="https://scholar.google.com/citations?user=0i-Nzv0AAAAJ&hl=en">Yu-Kun Lai</a>,
              <a href="https://scholar.google.com/citations?user=V5E7JXsAAAAJ&hl=en">Paul L. Rosin</a>
              <br>
              <em>Submitted to <a href="https://cvpr.thecvf.com/Conferences/2024/">CVPR 2024</a>, Under Review</em>
              <br>
              [<a href="">pdf</a>]
              <br>
              <p></p>
              <p>we propose AesStyler, a novel Aesthetic Guided Universal Style Transfer method, which utilizes pre-trained aesthetiic assessment model, a novel Universal Aesthetic Codebook and a novel Universal and Specific Aesthetic-Guided Attention (USAesA) module. Extensive experiments and user-studies have demonstrated that our approach generates aesthetically more harmonious and pleasing results than the state-of-the-art methods.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/icassp.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href = ""><papertitle>SAMVG: A Multi-stage Image Vectorization Model with the Segment-Anything Model</papertitle></a></papertitle>
              </a>
              <br>
              <strong>Haokun Zhu*</strong>,
              <a href="https://tsb0601.github.io/petertongsb/">Juang Ian Chong*</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=Jm5qsAYAAAAJ/">Teng Hu</a>,
              <a href="https://yiranran.github.io/">Ran Yi</a>,
              <a href="https://scholar.google.com/citations?user=0i-Nzv0AAAAJ&hl=en">Yu-Kun Lai</a>,
              <a href="https://scholar.google.com/citations?user=V5E7JXsAAAAJ&hl=en">Paul L. Rosin</a>
              <br>
              <em>Submitted to <a href="https://2024.ieeeicassp.org/">ICASSP 2024</a>, Under Review</em>
              <br>
              [<a href="">arXiv</a>]
              <br>
              <p></p>
              <p>In this paper, we propose SAMVG, a multi-stage model to vectorize raster images into SVG (Scalable Vector Graphics). Through a series of extensive experiments, we demonstrate that SAMVG can produce high quality SVGs in any domain while requiring less computation time and complexity compared to previous state-of-the-art methods.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mm.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611766"><papertitle>Stroke-based Neural Painting and Stylization with Dynamically Predicted Painting Region</papertitle></a>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?hl=en&user=Jm5qsAYAAAAJ/">Teng Hu</a>,
              <a href="https://yiranran.github.io/">Ran Yi</a>,
              <strong>Haokun Zhu</strong>,
              <a href="https://orcid.org/0000-0001-7910-810X/">Liang Liu</a>,
              <a href="https://orcid.org/0009-0003-1887-6406/">Jinlong Peng</a>,
              <a href="https://orcid.org/0000-0002-6592-8411/">Yabiao Wang</a>,
              <a href="https://orcid.org/0000-0003-4216-8090/">Chengjie Wang</a>
              <a href="https://orcid.org/0000-0003-1653-4341/">Lizhuang Ma</a>
              <br>
              <em>Accepted by ACM MM 2023</em>
              <br>
              [<a href="https://dl.acm.org/doi/pdf/10.1145/3581783.3611766">pdf</a>] [<a href="https://github.com/sjtuplayer/Compositional_Neural_Painter">code</a>] [<a href="https://arxiv.org/abs/2309.03504">arXiv</a>]
              <br>
              <p></p>
              <p>we propose Compositional Neural Painter, a novel stroke-based rendering framework which dynamically predicts the next painting region based on the current canvas, instead of dividing the image plane uniformly into painting regions. Extensive experiments show our model outperforms the existing models in stroke-based neural painting.</p>
            </td>
            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iccv.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_paper.html"><papertitle>Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption</papertitle></a>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?hl=en&user=Jm5qsAYAAAAJ/">Teng Hu</a>,
              <a href="https://zhangzjn.github.io/">Jiangning Zhang</a>,
              <a href="https://orcid.org/0000-0001-7910-810X/">Liang Liu</a>,
              <a href="https://yiranran.github.io/">Ran Yi</a>,
              <a href="https://github.com/karrykkk">Siqi Kou</a>,
              <strong>Haokun Zhu</strong>,

              <a href="https://scholar.google.com/citations?hl=zh-CN&user=1621dVIAAAAJ">Xu Chen</a>,
              <a href="https://orcid.org/0000-0002-6592-8411/">Yabiao Wang</a>,
              <a href="https://orcid.org/0000-0003-4216-8090/">Chengjie Wang</a>
              <a href="https://orcid.org/0000-0003-1653-4341/">Lizhuang Ma</a>
              <br>
              <em>Accepted by ICCV 2023</em>
              <br>
              [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_paper.pdf">pdf</a>] [<a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_supplemental.pdf">supp</a>] [<a href="https://github.com/sjtuplayer/few-shot-diffusion">code</a>] [<a href="https://arxiv.org/abs/2309.03729">arXiv</a>]
              <br>
              <p></p>
              <p>we propose a novel phasic content fusing few-shot diffusion model with directional distribution consistency loss, which targets different learning objectives at distinct training stages of the diffusion model. Theoretical analysis, and experiments demonstrate the superiority of our approach in few-shot generative model adaption tasks.</p>
            </td>
          </tr>
          </tr>

          <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/t10ssl.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <papertitle> Pushing the Limit of Training Efficiency in Self-Supervised Learning </papertitle>
              </a>
              <br>
              <strong>Shengbang Tong*</strong>, <a>Yubei Chen*</a>, <a>Yi Ma</a>, <a>Yann Lecun</a>
              <br>
              <em>Under Review</em>
              <br>
              <p></p>
              <p>Inspired by the newly proposed principle, our work proposes a minimalist method for self-supervised learning that tremendously reduces the epochs which SSL methods take to converge.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/uCTRL.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2210.16782" id="uctrl">
                <papertitle>Unsupervised Learning of Structured Representation via Closed-Loop Transcription</papertitle>
              </a>
              <br>
              <strong>Shengbang Tong*</strong>, <a href = "https://delay-xili.github.io/">Xili Dai*</a>, <a>Yubei Chen</a>, <a>Mingyang Li</a>, <a>Zengyi Li</a>,  <a>Brent Yi</a>, <a>Yann Lecun</a>, <a>Yi Ma</a>
              <br>
              <em>Under Review</em>
              <br>
              <p></p>
              <p>This paper proposes a new unsupervised method to learn a structured representation that may serve both discriminative and generative purpose</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CSC_CTRL.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <papertitle> Closed-Loop Transcription Via Convolutional Sparse Coding</papertitle>
              </a>
              <br>
              <a>Xili Dai*</a>, <a>Ke Chen*</a>, <strong>Shengbang Tong*</strong>, <a>Jingyuan Zhang*</a>, <a>Xingjian Gao</a>, <a>Mingyang Li</a>, <a>Druv Pai</a>, <a>Yuexiang Zhai</a>, <a>Xiaojun Yuan</a>, <a>Heung Yeung Shum</a>, <a>Lionel M.Ni</a>, <a>Yi Ma</a>
              <br>
              <em>Under Review</em>
              <br>
              <p></p>
              <p>This paper proposes a new unsupervised method to learn a represenation and cluster for real life dataset such as CIFAR-10, CIFAR100 and Tiny-ImageNet-200.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/MCRSE.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2301.01805" id="umlc">
                <papertitle>Unsupervised Manifold Linearizing and Clustering</papertitle>
              </a>
              <br>
              <a>Tianjiao Ding</a>, <strong>Shengbang Tong</strong>, <a>Kwan Ho Ryan Chan</a>, <a href = "https://delay-xili.github.io/">Xili Dai</a>, <a>Yi Ma</a>,<a>Benjamin David Haeffele</a>
              <br>
              <em>Under Review</em>
              <br>
              <p></p>
              <p>This paper proposes a new unsupervised method to learn a represenation and cluster for real life dataset such as CIFAR-10, CIFAR100 and Tiny-ImageNet-200.</p>
            </td>
          </tr>




          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sdnet.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2210.12945" id="revisit">
                <papertitle>Revisiting Sparse Convolutional Model for Visual Recognition</papertitle>
              </a>
              <br>
              <a href = "https://delay-xili.github.io/">Xili Dai*</a>,  <a>Mingyang Li*</a>, <a>Pengyuan Zhai</a>,  <strong>Shengbang Tong</strong>, <a>Xingjian Gao</a>, <a>Shaolun Huang</a>, <a>Zhihui Zhu</a>, <a>Chong You</a>, <a>Yi Ma</a>
              <br>
              <em>Accepted by Nips 2022</em>
              <br>
              <p></p>
              <p>Our method uses differentiable optimization layers that are defined from convolutional sparse coding as drop-in replacements of standard convolutional layers in conventional deep neural networks. We show that such models have equally strong empirical performance on CIFAR-10, CIFAR-100 and ImageNet datasets when compared to conventional neural networks.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iLDR.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2202.05411" id="iLDR">
                <papertitle>Incremental Learning of Structured Memory via Closed-Loop Transcription</papertitle>
              </a>
              <br>
              <strong>Shengbang Tong</strong>, <a href = "https://delay-xili.github.io/">Xili Dai</a>, <a>Ziyang Wu</a>, <a>Mingyang Li</a>, <a>Brent Yi</a>, <a>Yi Ma</a>
              <br>
              <em>Accepted by ICLR 2023</em>
              <br>
              <p></p>
              <p>We propose a minimal computational model for learning a structured memory of multiple object classes in an incremental setting</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/LDR.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2111.06636" id="LDR">
                <papertitle>Closed-Loop Data Transcription to an LDR via Minimaxing Rate Reduction</papertitle>
              </a>
              <br>
              <a href = "https://delay-xili.github.io/">Xili Dai*</a>, <strong>Shengbang Tong*</strong>, <a>Mingyang Li*</a>, <a>Ziyang Wu*</a>, <a>Kwan Ho Ryan Chan</a>, <a>Pengyuan Zhai</a>, <a>Yaodong Yu</a>, <a>Michael Psenka</a>, <a>Xiaojun Yuan</a>, <a>Heung Yeung Shum</a>, <a>Yi Ma</a>
              <br>
              <em>Accepted by Entropy Journal</em>
              <br>
              <p></p>
              <p>We propose a new computational framework for learning an explicit generative model for real-world dataset.</p>
            </td>
          </tr> -->



        <!-- </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Projects</heading>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/nerf.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/LeslieTrue/" id="revisit">
                <papertitle>(Undergoing, placeholder) Reimplementation of Instant NGP</papertitle>
              </a>
              <br>
              Paper by M√ºller et al. Reimplementation by <strong>Tianzhe Chu</strong>.
              <br>
              <em>Just for fun</em>
              <br>
              <p></p>
              <p>Instant-ngp via pytorch or Jax or both.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cg.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://leslietrue.github.io/cs184-project-webpage/" id="revisit">
                <papertitle>(Undergoing) Computer Graphics</papertitle>
              </a>
              <br>
              <strong>Tianzhe Chu</strong>
              <br>
              <em>CS 184 Computer Graphics Course Project</em>
              <br>
              <p></p>
              <p>It's a perfect walkthrough of fondamental knowledge in CG.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rt.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://github.com/LeslieTrue/cs285_Project" id="revisit">
                <papertitle>Reward Transformer</papertitle>
              </a>
              <br>
              <strong>Tianzhe Chu</strong>
              <br>
              <em>CS 285 Deep Reinforcement Learning Course Project</em>
              <br>
              <p></p>
              <p>This method proposes a new way to learn the reward distribution of an environment.</p>
            </td>
          </tr> -->
          <table class="sub-table" style="width: 200px;height: 100px;" align="center">
            <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=e7UZEtJzHSskg2uAm2OcvYFvUik7x45QuD4lLLv2j-E&cl=ffffff&w=a"></script>
          </table>
        </tbody>
        </table>



      </td>
    </tr>
  </table>
</body>

</html>
